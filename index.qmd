---
title: "PCA (principal component analysis)"
author: "Brandy, Lee, Tavish"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
    highlight: pygments
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
editor: 
  markdown: 
    wrap: 72
---

```{=html}
<style>
 .title {
    color: #191970;    
  }
 .subtitle {
    color: #191970;
  }
  .author {
    color: #191970;
}
 .date {
    color: #191970;
  }
 body {
    background-color: #FAFAF5;
  }
</style>
```
## [Introduction]{style="color: #191970;"}

[**1. A comparative dimensionality reduction study in telecom customer
segmentation using deep learning and PCA**]{style="color: #8B814C;"}

[This article compares PCA with a deep learning autoencoder on telecom
customer data. Goal is to segment customers based on 220 features to
optimize customer satisfaction/loyalty. Raw data must be cleaned &
standardized (z-score normalization) before applying PCA. A scree plot
was used to visualize the number of features to keep, which was 3 that
explain \~72% of data. Using the absolute values of the eigenvectors
they can decide wich features contribute the most to the first 3 PC's.
Overall PCA saved \~90% of total variance with just 20 features,
reducing original dataset by 200. [@Alkha]]{style="color: #8B814C;"}

[**2. Visualizing Psychological Networks: A Tutorial in
R**]{style="color: #8B814C;"}

[Article aims to guide researchers on choosing the best/most
interpretable visualizations of psychological networks. These networks
include mental disorder symptoms (specifically OCD and depression) and
the connections/correlations between them. When plotting, the symptoms
are referred to as nodes and their connections as edges. The article
goes on to compare 4 different plotting approaches; force-directed
algorithms, multidimensional scaling, PCA, and eigenmodels. 6 different
benefits were used as a checklist for all 4 approches. Benefits include:
node placement is meaningful, useful for comparing replications,
distances between nodes is interpretable, X/Y placement of nodes is
interpretable, can be based on any network, central nodes in the center.
PCA checks 3 of the 6 benefits, node placement is meaningful, useful for
comparing replications, and X/Y placement of nodes is interpretable
(this being the primary benefit since nodes/symptoms can be compared
right to left, X, or top from bottom Y). 1 major disadvantage of PCA is
that it relies on the correlation matrix and nodes/symptoms can be
difficult to see when they score similarly on both PC's.
[@Jones]]{style="color: #8B814C;"}

[**3. SVM and PCA Based Learning Feature Classification Approaches for
E-Learning System**]{style="color: #8B814C;"}

[This article aims to classify student learning attributes using PCA in
order to develop a simple methodology to optimize a students dynamic
learning sequence based on their individual skill, needs and
preferences, and also to maximize their learning outcome in computer
programming courses (java, C). The study uses 8 different learning
attributes; anxiety, personality, learning style, cognitive style,
grades from prev. semester, motivation, study level, and student prior
knowledge. 100 students taking C programming course were used in the
present study. Each student filled out a questionairre to gather
information on the 8 learning attributes. They each took a 20 question
midterm and also a final that scored their capabilities in 3 areas;
syntax, logical, and application (each score was divided into low, med,
and high categories). For this study, 3 PC's were kept & used for
classification purposes and explain \~77% of the variance. From figure 5
it looks like the first PC is comprised of 3 main learning attributes
prior knowledge, learning style, and personality. Second PC groups
motivation, cognitive style, and grades from prev. semester. Third PC
groups anxiety with study level. The article then goes on to use the 3
PC's to fit 4 classification models, a neural network, quadratic SVM,
gaussian SVM, and linear SVM. Linear SVM provided the highest accuracy,
sensitivity, and specificity, outperforming the other kernal
classifiers. [@Khamp]]{style="color: #8B814C;"}

[**4. Applied Multivariate Statistical Analysis and Related Topics with
R**]{style="color: #8B814C;"}

[This chapter goes over PCA basic's and how and why it is useful. The
overall goal of PCA is to minimize the number of predictive variables
while maintaining most of the variation (70%\~80% as a guideline, the
book goes on to cover a few more rules of thumb for this). PCA is a good
method for finding outliers since the principal components are linear
combinations of the original data - plotting with lower dimensions makes
them easier to spot. PCA is also useful when some of the predictor
variables in a regression model are highly correlated, which if not
addressed can lead to poor parameter estimates. The first PC explains
the most variability in the data & each succeeding PC explains the most
possible remaining variability. PCA analysis should be preformed on
scaled/unit data, magnitudes must be comparable (use the correlation
matrix instead of covariance OR standardize the data).
[@lang2021applied]]{style="color: #8B814C;"}

[**5. The fixed effects PCA model in a common principal component
environment**]{style="color: #8B814C;"}

[The article compares a fixed effects PCA model to the 2 most common
approaches, descriptive algebraic and probabilistic. All three produce
the same results using spectral decomposition of the sample covariance
matrix but, the interpretations will differ depending on the
assumptions. Graphing the low dimensional PC's (usually the first 2) is
a common way to identify hidden patterns in the data such as outliers or
clusters. The fixed effects model only makes assumptions about the
dimensionality of the data, and incorporates knowledge about noise in
the data to improve estimates. The results of the paper were that the
fixed effects model incorporating CPCA (common PCA) out preformed all
others. [@duras2022fixed]]{style="color: #8B814C;"}

[**6. Principal component analysis**]{style="color: #8B814C;"}

[This is a brief article discussing the accumulation of error when applying PCA to large datasets and how to locate important patterns when using the method and how scaling can affect the dataset being analyzed. It provides several graphs and figures.
]style="color: #8B814C;"}

[**7. PCAtest: testing the statistical significance of Principal Component Analysis in R**]{style="color: #8B814C;"}

[This article begins with a brief history of PCA and then moves on to discuss an R package known as PCAtest which implements permutation-based statistical tests to evaluate the overall significance of a PCA. The article describes the process that PCAtest implements: a bootstrapping procedure that calculates various statistics related to PCA. From there the article discusses the results of an example dataset that has been analyzed using PCA and then that PCA analyzed using PCAtest. Visuals and graphical aids are included. Finally the article concludes by discussing when to use PCA and how to interpret the results. 
]style="color: #8B814C;"}

[**8. An Overview of Principal Component Analysis**]{style="color: #8B814C;"}

[This article is a brief overview of PCA and describes the methods used throughout the process of performing PCA while mentioning its application in facial recognition software and then mentioning some advantages and disadvantages of using the method.
]style="color: #8B814C;"}

[**9. Exploration of Principal Component Analysis: Deriving Principal Component Analysis Visually Using Spectra**]{style="color: #8B814C;"}

[This article introduces PCA and then goes on to describe methods and providing examples of PCA while using spectral graphs as visual aids. A brief description of the matrix theory behind PCA is analyzed through the use of spectral graphs and finally instructions on how to interpret results from PCA is given as a general model.
]style="color: #8B814C;"}

[**10. Principal component analysis: a review and recent developments**]{style="color: #8B814C;"}

[This article describes PCAâ€™s usefulness in data science and in dealing with large datasets and from there gives a basic description of the methods involved in PCA followed by an example. The article then proceeds to discuss some issues related to PCA while providing plots and visuals and then examines some adaptations of PCA with examples included.]style="color: #8B814C;"}

## [Methods]{style="color: #191970;"}

## [Analysis and Results]{style="color: #191970;"}

## [Data and Vizualisation]{style="color: #191970;"}

[Bacteria dataset from MASS package (220 rows by 6
columns)]{style="color: #8B814C;"}

[Dr A. Leach tested the effects of a drug on 50 children with a history
of otitis media in the Northern Territory of Australia. The children
were randomized to the drug or the a placebo, and also to receive active
encouragement to comply with taking the drug. The presence of H.
influenzae was checked at weeks 0, 2, 4, 6 and 11: 30 of the checks were
missing and are not included in this data
frame.]{style="color: #8B814C;"}

[**Variables**]{style="color: #8B814C;"}

[**y** - presence or absence: a factor with levels n and
y]{style="color: #8B814C;"}

[**ap** - active/placebo: a factor with levels a and
p]{style="color: #8B814C;"}

[**hilo** - hi/low compliance: a factor with levels hi amd
lo]{style="color: #8B814C;"}

[**week** - numeric: week of test]{style="color: #8B814C;"}

[**ID** - subject ID: a factor]{style="color: #8B814C;"}

[**trt** - a factor with levels placebo, drug and drug+, a re-coding of
ap and hilo]{style="color: #8B814C;"}

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(MASS)

```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(bacteria))

#ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  #ggplot1 + geom_point(aes(col=region), size = 4) +
  #geom_text_repel(aes(label=abb)) +
  #scale_x_log10() +
  #scale_y_log10() +
  #geom_smooth(formula = "y~x", method=lm,se = F)+
  #xlab("Populations in millions (log10 scale)") + 
  #ylab("Total number of murders (log10 scale)") +
  #ggtitle("US Gun Murders in 2010") +
  #scale_color_discrete(name = "Region")+
  #    theme_wsj()

```

## [Statistical Modeling]{style="color: #191970;"}

## [Conlusion]{style="color: #191970;"}

## [References]{style="color: #191970;"}
